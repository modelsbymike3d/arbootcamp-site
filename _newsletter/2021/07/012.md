---
path: "2021/07/ar-bootcamp-12"
image: ""
title: "Newsletter #12"
date: "2021-07-30"
---

## Tutorial dump

In the last issue I mentioned that I was working on a style transfer tutorial using SnapML, and I'm glad to say I finally got it made. I also made "Big Head" and inpainting tutorials for Lens Studio.

- [SnapML Style Transfer](https://www.youtube.com/watch?v=S7T3dSk4LK8)
- [Big Head](https://www.youtube.com/watch?v=EoF2eQnYjT8)
- [Inpainting](https://www.youtube.com/watch?v=Xq9CVKOOQLY)

## SplitCam and Snap Camera fun

I was inspired by some stuff I saw on [Twitter](https://twitter.com/CitizenPlain/status/1417294943329132544) to mess around with Snap Camera some more and I figured out how to use SplitCam to send any [arbitrary video into Snap Camera](https://www.youtube.com/watch?v=0zoZ-Ap4FHc). It's pretty fun to add lenses to existing videos.

## Snap policy updates

A ton of creators add messages like "subscribe to me for more filters" to their Snapchat lenses. There is nothing against that currently listed in the Lens Studio submission guidelines, but apparently they decided they don't like that sort of thing anymore. I know a good number of creators were having fun updating their lenses to remove those sorts of messages. And about a week before all this went down I of course updated my tutorial on how to do it.

## Trending predictions

Lens Studio 4.1 now includes an inpainting effect in the Asset Library. Believe it or not, there is no machine learning going on - it appears to entirely be done with copying and blurring what the camera sees. It isn't perfect, but it does a good enough job that it is a blast to use. It's main use case I think is to remove people from the scene when they are part of the background (like a 3D body tracking lens), but it's also a ton of fun to use as the main feature. I think we are going to be seeing a lot of creative uses for inpainting.

## Spark AR?

When I first created the AR Bootcamp website I was really wanting to devote more space to Spark AR, but I just have a hard time getting excited about it when I see all the new features in Lens Studio. The only brand new feature in v118 is antialiasing. Really? I don't understand how they can have [10,000 employees working on XR](https://www.xrtoday.com/virtual-reality/xr-tech-now-accounts-for-almost-20-of-facebooks-workforce/) and yet be falling behind so bad with Spark AR. I'm almost certain they have some really talented developers, so I'm going to posit a guess and say they are mismanaging their AR platform. Sharing effects between Facebook, Instagram, and Portal is also probably a big limiting factor if their camera code is developed separately for the various platforms (which I'm guessing it is).

Do any of you work for Facebook or know anyone who does? I'd love to know a little more about what's going with Spark AR.
